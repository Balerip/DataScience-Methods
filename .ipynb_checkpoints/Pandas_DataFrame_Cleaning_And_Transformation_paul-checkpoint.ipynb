{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:32:08.889756Z",
     "start_time": "2020-10-19T17:32:07.647730Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# Set some pandas options for controlling output\n",
    "\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get unique values in columns of a Dataframe\n",
    "\n",
    "<font color = 'red'>DataFrame.unique(self, axis=0, dropna=True)</font> # \n",
    "\n",
    "returns the unique elements along different axis.\n",
    "\n",
    "- If axis = 0 : It returns a series object containing the count of unique elements in each column.\n",
    "- If axis = 1 : It returns a series object containing the count of unique elements in each row.\n",
    "- Default value of axis is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:16:07.711291Z",
     "start_time": "2020-07-13T14:16:07.570995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the Dataframe : \n",
      "      Name   Age     City  Experience\n",
      "a     jack  34.0   Sydney           5\n",
      "b     Riti  31.0    Delhi           7\n",
      "c     Aadi  16.0      NaN          11\n",
      "d    Mohit  31.0    Delhi           7\n",
      "e    Veena   NaN    Delhi           4\n",
      "f  Shaunak  35.0   Mumbai           5\n",
      "g    Shaun  35.0  Colombo          11\n"
     ]
    }
   ],
   "source": [
    "# List of Tuples\n",
    "employees = [('jack', 34, 'Sydney', 5) ,\n",
    "('Riti', 31, 'Delhi' , 7) ,\n",
    "('Aadi', 16, np.NaN, 11) ,\n",
    "('Mohit', 31,'Delhi' , 7) ,\n",
    "('Veena', np.NaN, 'Delhi' , 4) ,\n",
    "('Shaunak', 35, 'Mumbai', 5 ),\n",
    "('Shaun', 35, 'Colombo', 11)\n",
    "]\n",
    "# Create a DataFrame object\n",
    "emp_df = pd.DataFrame(employees, columns=['Name', 'Age', 'City', 'Experience'], index=['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n",
    "print(\"Contents of the Dataframe : \")\n",
    "print(emp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'Age', 'City', 'Experience'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name           object\n",
       "Age           float64\n",
       "City           object\n",
       "Experience      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:16:28.550487Z",
     "start_time": "2020-07-13T14:16:28.538063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jack' 34.0 'Sydney' 5 'Riti' 31.0 'Delhi' 7 'Aadi' 16.0 nan 11 'Mohit'\n",
      " 31.0 'Delhi' 7 'Veena' nan 'Delhi' 4 'Shaunak' 35.0 'Mumbai' 5 'Shaun'\n",
      " 35.0 'Colombo' 11]\n",
      "['jack' 34.0 'Sydney' 5 'Riti' 31.0 'Delhi' 7 'Aadi' 16.0 nan 11 'Mohit'\n",
      " 'Veena' 4 'Shaunak' 35.0 'Mumbai' 'Shaun' 'Colombo']\n"
     ]
    }
   ],
   "source": [
    "# Get a series of unique values of the dataframe\n",
    "\n",
    "column_values = emp_df.values.ravel()\n",
    "unique_values =  pd.unique(column_values)\n",
    "print(column_values)\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34., 31., 16., nan, 35.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(emp_df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:17:22.281437Z",
     "start_time": "2020-07-13T14:17:22.272886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique elements in column \"Age\" \n",
      "[34. 31. 16. nan 35.]\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Find unique values in a single column\n",
    "\n",
    "'''\n",
    "\n",
    "# Get a series of unique values in column 'Age' of the dataframe\n",
    "uniqueValues = emp_df['Age'].unique()\n",
    " \n",
    "print('Unique elements in column \"Age\" ')\n",
    "print(uniqueValues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get count of unique values in columns of a Dataframe\n",
    "<font color = 'red'>DataFrame.nunique(self, axis=0, dropna=True)</font>\n",
    "\n",
    "It returns the count of unique elements along different axis.\n",
    "\n",
    "- If axis = 0 : It returns a series object containing the count of unique elements in each column.\n",
    "- If axis = 1 : It returns a series object containing the count of unique elements in each row.\n",
    "- Default value of axis is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:18:34.578823Z",
     "start_time": "2020-07-13T14:18:34.569503Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in column \"Age\" of the dataframe : \n",
      "4\n"
     ]
    }
   ],
   "source": [
    "'''Count unique values in a single column, it ignores NA values'''\n",
    "\n",
    "# Count unique values in column 'Age' of the dataframe\n",
    "uniqueValues_count = emp_df['Age'].nunique()\n",
    " \n",
    "print('Number of unique values in column \"Age\" of the dataframe : ')\n",
    "print(uniqueValues_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:19:56.733292Z",
     "start_time": "2020-07-13T14:19:56.719801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in column \"Age\" including NaN\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Include NaN while counting the unique elements in a column\n",
    "\n",
    "\n",
    "'''\n",
    "# Count unique values in column 'Age' including NaN\n",
    "uniqueValues_withna = emp_df['Age'].nunique(dropna=False)\n",
    " \n",
    "print('Number of unique values in column \"Age\" including NaN')\n",
    "print(uniqueValues_withna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:20:34.033823Z",
     "start_time": "2020-07-13T14:20:33.912155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique values in each column :\n",
      "Name          7\n",
      "Age           4\n",
      "City          4\n",
      "Experience    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Count unique values in each column of the dataframe\n",
    "\n",
    "'''\n",
    "\n",
    "# Get a series object containing the count of unique elements\n",
    "# in each column of dataframe\n",
    "uniqueValues_df = emp_df.nunique()\n",
    " \n",
    "print('Count of unique values in each column :')\n",
    "print(uniqueValues_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:23:41.532851Z",
     "start_time": "2020-07-13T14:23:41.522160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Unique values in each column including NaN\n",
      "Name          7\n",
      "Age           5\n",
      "City          5\n",
      "Experience    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count unique elements in each column including NaN\n",
    "uniqueValues_withna = emp_df.nunique(dropna=False)\n",
    " \n",
    "print(\"Count Unique values in each column including NaN\")\n",
    "print(uniqueValues_withna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:24:04.574116Z",
     "start_time": "2020-07-13T14:24:04.563789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jack' 34.0 'Riti' 31.0 'Aadi' 16.0 'Mohit' 31.0 'Veena' nan 'Shaunak'\n",
      " 35.0 'Shaun' 35.0]\n"
     ]
    }
   ],
   "source": [
    "# Get Unique values in a multiple columns\n",
    "column_values = emp_df[[\"Name\", \"Age\"]].values.ravel() # CHECK IT \n",
    "#unique_values =  pd.unique(column_values)\n",
    "print(column_values)\n",
    "#print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:44.300843Z",
     "start_time": "2020-07-13T02:37:44.289998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique elements in column \"Name\" & \"Age\" :\n",
      "['jack' 'Riti' 'Aadi' 'Mohit' 'Veena' 'Shaunak' 'Shaun' 34.0 31.0 16.0 nan\n",
      " 35.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/b2rs611d1g96lldn5x1gj1wm0000gp/T/ipykernel_4223/1061482607.py:12: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  uniqueValues = (emp_df['Name'].append(emp_df['Age'])).unique()\n"
     ]
    }
   ],
   "source": [
    "'''Get Unique values in a multiple columns\n",
    "\n",
    "\n",
    "To get the unique values in multiple columns of a dataframe, we can merge the \n",
    "contents of those columns to \n",
    "create a single series object and then can call unique() function \n",
    "on that series object\n",
    "\n",
    "'''\n",
    "\n",
    "# Get unique elements in multiple columns i.e. Name & Age\n",
    "uniqueValues = (emp_df['Name'].append(emp_df['Age'])).unique()\n",
    "print('Unique elements in column \"Name\" & \"Age\" :')\n",
    "print(uniqueValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data\n",
    "Missing data is a problem in real life scenarios. Areas like machine learning and data mining face severe issues in the accuracy of their model predictions because of poor quality of data caused by missing values. In these areas, missing value treatment is a major point of focus to make their models more accurate and valid.\n",
    "\n",
    "In Pandas missing data is represented by two value:\n",
    "\n",
    "- **None**: None is a Python singleton object that is often used for missing data in Python code.\n",
    "\n",
    "- **NaN**: NaN (an acronym for Not a Number), is a special floating-point value recognized by all systems that use the standard IEEE floating-point representation\n",
    "\n",
    "Pandas treat *None* and *NaN* as essentially interchangeable for indicating missing or null values. To facilitate this convention, there are several useful functions for detecting, removing, and replacing null values in Pandas DataFrame:\n",
    "\n",
    "- <font color = 'red'>isnull()</font>\n",
    "- <font color = 'red'> notnull()</font>\n",
    "- <font color = 'red'>dropna()</font>\n",
    "- <font color = 'red'>fillna()</font>\n",
    "- <font color = 'red'>replace()</font>\n",
    "- <font color = 'red'>interpolate()</font>\n",
    "\n",
    "\n",
    "#### Handling Missing Values\n",
    "\n",
    "- **Ignore the Missing Value During Analysis**:  This is usually done when class label is missing ( assuming the mining task involves classification). This method is not very effective, unless the record contains several attributes with missing values. It is especially poor when the percentage of missing values per attribute varies considerably.\n",
    "\n",
    "\n",
    "- **Fill in the missing value manually**: In general, this approach is time consuming and may not be feasible given a large data set with many missing values\n",
    "\n",
    "\n",
    "- **Use a global constant/mean or median to fill in the missing value**: Replace all missing feature values by the same constant/mean or median of the attribute\n",
    "\n",
    "\n",
    "- **Use the most probable value to fill in the missing value**: This may be determined with regression, inference-based tools using a Bayesian formalism, or decision tree induction. \n",
    "\n",
    "\n",
    "**Note** it is as important to avoid adding bias and distortion\n",
    "to the data as it is to make the information available.\n",
    "\t\n",
    "<font color = 'red'> bias is added when a wrong value is filled-in </font>\n",
    "\n",
    "No matter what techniques you use to conquer the problem, it\n",
    "comes at a price. The more guessing you have to do, the further\n",
    "away from the real data the database becomes. Thus, in turn, it\n",
    "can affect the accuracy and validation of the mining results. \n",
    "\n",
    "\n",
    "Here we will see how we can handle missing values using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:28:31.278449Z",
     "start_time": "2020-07-13T14:28:31.245780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data frame:\n",
      "        one       two     three\n",
      "a -0.009315  1.260082  0.179463\n",
      "c  0.735934 -0.712741  0.737624\n",
      "e -0.129400 -0.436079 -0.369286\n",
      "f  1.229614  1.287228 -0.346845\n",
      "h -0.667570  2.584569 -1.205152\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randn(5, 3), index=['a', 'c', 'e', 'f',\n",
    "'h'],columns=['one', 'two', 'three'])\n",
    "print(\"original data frame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:28:31.278449Z",
     "start_time": "2020-07-13T14:28:31.245780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "modified data frame with missing values:\n",
      "        one       two     three\n",
      "a -0.009315  1.260082  0.179463\n",
      "b       NaN       NaN       NaN\n",
      "c  0.735934 -0.712741  0.737624\n",
      "d       NaN       NaN       NaN\n",
      "e -0.129400 -0.436079 -0.369286\n",
      "f  1.229614  1.287228 -0.346845\n",
      "g       NaN       NaN       NaN\n",
      "h -0.667570  2.584569 -1.205152\n"
     ]
    }
   ],
   "source": [
    "# It will introduce NA values where indices are not present in the original\n",
    "df = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n",
    "print(\"\\n\")\n",
    "print(\"modified data frame with missing values:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8 entries, a to h\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   one     5 non-null      float64\n",
      " 1   two     5 non-null      float64\n",
      " 2   three   5 non-null      float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 256.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing values\n",
    "\n",
    "Pandas provides the <font color = 'red'>isnull</font> and <font color = 'red'>notnull()</font> functions.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:30:54.215858Z",
     "start_time": "2020-07-13T14:30:54.204676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     one    two  three\n",
      "a  False  False  False\n",
      "b   True   True   True\n",
      "c  False  False  False\n",
      "d   True   True   True\n",
      "e  False  False  False\n",
      "f  False  False  False\n",
      "g   True   True   True\n",
      "h  False  False  False\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:31:21.087763Z",
     "start_time": "2020-07-13T14:31:21.078051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    False\n",
      "b     True\n",
      "c    False\n",
      "d     True\n",
      "e    False\n",
      "f    False\n",
      "g     True\n",
      "h    False\n",
      "Name: one, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# we can do one column also\n",
    "\n",
    "print(df['one'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:31:58.464597Z",
     "start_time": "2020-07-13T14:31:58.456637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Any missing values?\n",
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:32:09.371327Z",
     "start_time": "2020-07-13T14:32:09.357235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().values.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        one       two     three\n",
       "a -0.009315  1.260082  0.179463\n",
       "b       NaN       NaN       NaN\n",
       "c  0.735934 -0.712741  0.737624\n",
       "d       NaN       NaN       NaN\n",
       "e -0.129400 -0.436079 -0.369286\n",
       "f  1.229614  1.287228 -0.346845\n",
       "g       NaN       NaN       NaN\n",
       "h -0.667570  2.584569 -1.205152"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:35:19.156724Z",
     "start_time": "2020-07-13T14:35:19.147215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one      3\n",
      "two      3\n",
      "three    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of missing values in each column\n",
    "\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:36:06.045075Z",
     "start_time": "2020-07-13T14:36:05.982096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one      0.375\n",
      "two      0.375\n",
      "three    0.375\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage\n",
    "percent_na = df.isnull().sum()/len(df)\n",
    "#percent_na = df.isnull().sum()/(df.shape[0])\n",
    "print(percent_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:38:08.788833Z",
     "start_time": "2020-07-13T14:38:08.772259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of missing values\n",
    "total_na = (df.isnull().sum().sum())\n",
    "print(total_na)\n",
    "print(total_na/df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        one       two     three\n",
       "a -0.009315  1.260082  0.179463\n",
       "b       NaN       NaN       NaN\n",
       "c  0.735934 -0.712741  0.737624\n",
       "d       NaN       NaN       NaN\n",
       "e -0.129400 -0.436079 -0.369286\n",
       "f  1.229614  1.287228 -0.346845\n",
       "g       NaN       NaN       NaN\n",
       "h -0.667570  2.584569 -1.205152"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1,1]= 0.983\n",
    "df.iloc[6,2]= 0.758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        one       two     three\n",
       "a -0.009315  1.260082  0.179463\n",
       "b       NaN  0.983000       NaN\n",
       "c  0.735934 -0.712741  0.737624\n",
       "d       NaN       NaN       NaN\n",
       "e -0.129400 -0.436079 -0.369286\n",
       "f  1.229614  1.287228 -0.346845\n",
       "g       NaN       NaN  0.758000\n",
       "h -0.667570  2.584569 -1.205152"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one      3\n",
       "two      2\n",
       "three    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:38:30.370938Z",
     "start_time": "2020-07-13T14:38:30.363199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    0\n",
      "b    2\n",
      "c    0\n",
      "d    3\n",
      "e    0\n",
      "f    0\n",
      "g    2\n",
      "h    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of missing values in each row\n",
    "print(df.isnull().sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        one       two     three\n",
       "a -0.009315  1.260082  0.179463\n",
       "b       NaN  0.983000       NaN\n",
       "c  0.735934 -0.712741  0.737624\n",
       "d       NaN       NaN       NaN\n",
       "e -0.129400 -0.436079 -0.369286\n",
       "f  1.229614  1.287228 -0.346845\n",
       "g       NaN       NaN  0.758000\n",
       "h -0.667570  2.584569 -1.205152"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the rows having *one = Null*  are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        one       two     three\n",
       "a -0.009315  1.260082  0.179463\n",
       "b       NaN  0.983000       NaN\n",
       "c  0.735934 -0.712741  0.737624\n",
       "d       NaN       NaN       NaN\n",
       "e -0.129400 -0.436079 -0.369286\n",
       "f  1.229614  1.287228 -0.346845\n",
       "g       NaN       NaN  0.758000\n",
       "h -0.667570  2.584569 -1.205152"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:44.515155Z",
     "start_time": "2020-07-13T02:37:44.496516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b   NaN\n",
       "d   NaN\n",
       "g   NaN\n",
       "Name: one, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[pd.isnull(df['one'])]\n",
    "df[pd.isnull(df['one'])]['one']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for missing values using** <font color = 'red'>notnull()</font>\n",
    "\n",
    "In order to check null values in Pandas Dataframe, we use <font color = 'red'>notnull()</font> function this function return dataframe of Boolean values which are False for NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     one    two  three\n",
       "a  False  False  False\n",
       "b   True  False   True\n",
       "c  False  False  False\n",
       "d   True   True   True\n",
       "e  False  False  False\n",
       "f  False  False  False\n",
       "g   True   True  False\n",
       "h  False  False  False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['one'].isnull()\n",
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:42:32.152535Z",
     "start_time": "2020-07-13T14:42:32.138130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     one    two  three\n",
       "a   True   True   True\n",
       "b  False   True  False\n",
       "c   True   True   True\n",
       "d  False  False  False\n",
       "e   True   True   True\n",
       "f   True   True   True\n",
       "g  False  False   True\n",
       "h   True   True   True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.notnull() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the rows having **one = Not Null**  are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:44.555409Z",
     "start_time": "2020-07-13T02:37:44.538464Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        one       two     three\n",
       "a -0.009315  1.260082  0.179463\n",
       "c  0.735934 -0.712741  0.737624\n",
       "e -0.129400 -0.436079 -0.369286\n",
       "f  1.229614  1.287228 -0.346845\n",
       "h -0.667570  2.584569 -1.205152"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get non-null values \n",
    "df[pd.notnull(df['one'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:44.591551Z",
     "start_time": "2020-07-13T02:37:44.564439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan in row 0 : 0\n",
      "Nan in row 1 : 2\n",
      "Nan in row 2 : 0\n",
      "Nan in row 3 : 3\n",
      "Nan in row 4 : 0\n",
      "Nan in row 5 : 0\n",
      "Nan in row 6 : 2\n",
      "Nan in row 7 : 0\n"
     ]
    }
   ],
   "source": [
    "''''Count total NaN at each row in DataFrame'''\n",
    "\n",
    "for i in range(len(df.index)) :\n",
    "    print(\"Nan in row\", i ,\":\",  df.iloc[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculations with Missing Data\n",
    "\n",
    "- When summing data, NA will be treated as **Zero**\n",
    "- If the data are all NA, then the result will be NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:43:36.057243Z",
     "start_time": "2020-07-13T14:43:36.050310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1592617215255872\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one column\n",
    "print(df['one'].sum())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:44.651025Z",
     "start_time": "2020-07-13T02:37:44.641980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one      1.159262\n",
      "two      4.966058\n",
      "three   -0.246196\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# DataFrame\n",
    "print(df.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Missing Values\n",
    "If you want to simply exclude the missing values, then use the <font color = 'red'>dropna</font> function along with the axis argument. By default, axis=0, i.e., along row, which means that if any value within a row is NA then the whole row is excluded.\n",
    "\n",
    "<font color = 'red'>DataFrame.dropna()</font>\n",
    "\n",
    "Python’s pandas library provides a function to remove rows or columns from a dataframe which contain missing values or NaN\n",
    "\n",
    "<font color = 'red'>DataFrame.dropna(self, axis=0, how='any', thresh=None, subset=None, inplace=False)</font>\n",
    "\n",
    "**Arguments** :\n",
    "\n",
    "- **axis**:\n",
    "    - 0 , to drop rows with missing values\n",
    "    - 1 , to drop columns with missing values\n",
    "- **how**:\n",
    "    - ‘any’ : drop if any NaN / missing value is present\n",
    "    - ‘all’ : drop if all the values are missing / NaN\n",
    "- **thresh**: threshold for non NaN values\n",
    "- **inplace** : If True then make changes in the dataplace itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:45:05.433181Z",
     "start_time": "2020-07-13T14:45:05.426150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:45:25.788041Z",
     "start_time": "2020-07-13T14:45:25.774682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        one       two     three\n",
      "a -0.009315  1.260082  0.179463\n",
      "c  0.735934 -0.712741  0.737624\n",
      "e -0.129400 -0.436079 -0.369286\n",
      "f  1.229614  1.287228 -0.346845\n",
      "h -0.667570  2.584569 -1.205152\n"
     ]
    }
   ],
   "source": [
    "print(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:46:09.252613Z",
     "start_time": "2020-07-13T14:46:09.229632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        one       two     three\n",
       "a -0.009315  1.260082  0.179463\n",
       "b       NaN  0.983000       NaN\n",
       "c  0.735934 -0.712741  0.737624\n",
       "d       NaN       NaN       NaN\n",
       "e -0.129400 -0.436079 -0.369286\n",
       "f  1.229614  1.287228 -0.346845\n",
       "g       NaN       NaN  0.758000\n",
       "h -0.667570  2.584569 -1.205152"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:47:18.877547Z",
     "start_time": "2020-07-13T14:47:18.861840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [a, b, c, d, e, f, g, h]\n"
     ]
    }
   ],
   "source": [
    "print(df.dropna(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:47:38.084306Z",
     "start_time": "2020-07-13T14:47:38.068773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  6.5  3.0\n",
       "1  1.0  NaN  NaN\n",
       "2  NaN  NaN  NaN\n",
       "3  1.0  6.5  3.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan],\n",
    "                     [np.nan, np.nan, np.nan], [1., 6.5, 3.]])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:48:06.556860Z",
     "start_time": "2020-07-13T14:48:06.539259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  6.5  3.0\n",
       "3  1.0  6.5  3.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = data.dropna()\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:48:54.564511Z",
     "start_time": "2020-07-13T14:48:54.533741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  6.5  3.0\n",
       "1  1.0  NaN  NaN\n",
       "3  1.0  6.5  3.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:44.789559Z",
     "start_time": "2020-07-13T02:37:44.770516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  6.5  3.0\n",
       "3  1.0  6.5  3.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing **how='all'** will only drop rows that are all NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:50:56.624410Z",
     "start_time": "2020-07-13T14:50:56.610901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(axis=1, how='any')  # axis = 1 drop columns\n",
    "#data.dropna(axis=1, how='all') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:44.833927Z",
     "start_time": "2020-07-13T02:37:44.815830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the Modified Dataframe : \n",
      "     0    1    2\n",
      "0  1.0  6.5  3.0\n",
      "3  1.0  6.5  3.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Drop Rows with any missing value in selected columns only\n",
    "\n",
    "'''\n",
    "mod_df = data.dropna(how = 'any', subset=[0,2])\n",
    "\n",
    "print(\"Contents of the Modified Dataframe : \")\n",
    "print(mod_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**thresh** Argument in the dropna() function\n",
    "\n",
    "Suppose you want to keep only rows containing a certain number of observations. You can indicate this with the thresh argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ct/b2rs611d1g96lldn5x1gj1wm0000gp/T/ipykernel_4223/391604064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:52:28.001727Z",
     "start_time": "2020-07-13T14:52:27.973903Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ct/b2rs611d1g96lldn5x1gj1wm0000gp/T/ipykernel_4223/2735654443.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#data.dropna(thresh = 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.dropna(thresh=1)\n",
    "#data.dropna(thresh = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling Missing data\n",
    "Rather than filtering out missing data (and potentially discarding other data along with it), you may want to fill in the “holes” in any number of ways. \n",
    "\n",
    "Pandas provides <font color = 'red'>fillna</font> function to fill in NA values with non-null data in various ways.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  6.5  3.0\n",
       "1  1.0  NaN  NaN\n",
       "2  NaN  NaN  NaN\n",
       "3  1.0  6.5  3.0"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:54:01.327890Z",
     "start_time": "2020-07-13T14:54:01.311442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN replaced with '0':\n",
      "     0    1    2\n",
      "0  1.0  6.5  3.0\n",
      "1  1.0  0.0  0.0\n",
      "2  0.0  0.0  0.0\n",
      "3  1.0  6.5  3.0\n"
     ]
    }
   ],
   "source": [
    "# Replace NaN with a scalar Value\n",
    "\n",
    "print (\"NaN replaced with '0':\")\n",
    "print(data.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:56:19.490230Z",
     "start_time": "2020-07-13T14:56:19.475816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  6.5  3.0\n",
       "1  1.0  NaN  0.0\n",
       "2  0.5  NaN  0.0\n",
       "3  1.0  6.5  3.0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill NA in particular columns with different values\n",
    "data.fillna({0: 0.5, 2: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T15:00:30.356657Z",
     "start_time": "2020-07-13T15:00:30.333844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  6.5  3.0\n",
       "1  1.0  NaN  NaN\n",
       "2  NaN  NaN  NaN\n",
       "3  1.0  6.5  3.0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NA Foward and Backward\n",
    "\n",
    "| Method |   Action   |\n",
    "|:---|:---|\n",
    "|   pad/fill | Fill methods forward   |\n",
    "|   bfill/backfill | Fill methods Backward    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T15:02:53.099910Z",
     "start_time": "2020-07-13T15:02:53.089572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2\n",
      "0  1.0  6.5  3.0\n",
      "1  1.0  6.5  3.0\n",
      "2  1.0  6.5  3.0\n",
      "3  1.0  6.5  3.0\n"
     ]
    }
   ],
   "source": [
    "print(data.fillna(method='pad')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T15:05:21.321067Z",
     "start_time": "2020-07-13T15:05:21.309695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  6.5  3.0\n",
       "1  1.0  NaN  NaN\n",
       "2  NaN  NaN  NaN\n",
       "3  1.0  6.5  3.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2\n",
      "0  1.0  6.5  3.0\n",
      "1  1.0  6.5  3.0\n",
      "2  1.0  6.5  3.0\n",
      "3  1.0  6.5  3.0\n"
     ]
    }
   ],
   "source": [
    "print(data.fillna(method='bfill')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        one       two     three\n",
       "a  1.354348  0.358138 -0.603686\n",
       "b  0.366249 -0.122000  0.018230\n",
       "c -0.251607  0.546859  0.360552\n",
       "d  0.366249 -0.122000  0.018230\n",
       "e  0.398737 -0.417704  0.267737\n",
       "f  0.621255  0.659855  0.824654\n",
       "g  0.366249 -0.122000  0.018230\n",
       "h -0.291490 -1.757148 -0.758105"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T15:06:07.173241Z",
     "start_time": "2020-07-13T15:06:07.155788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        one       two     three\n",
      "a  1.354348  0.358138 -0.603686\n",
      "b  0.366249 -0.122000  0.018230\n",
      "c -0.251607  0.546859  0.360552\n",
      "d  0.366249 -0.122000  0.018230\n",
      "e  0.398737 -0.417704  0.267737\n",
      "f  0.621255  0.659855  0.824654\n",
      "g  0.366249 -0.122000  0.018230\n",
      "h -0.291490 -1.757148 -0.758105\n"
     ]
    }
   ],
   "source": [
    "print(df.fillna(method='pad')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  6.5  3.0\n",
       "1  1.0  NaN  NaN\n",
       "2  NaN  NaN  NaN\n",
       "3  1.0  6.5  3.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T15:03:34.369988Z",
     "start_time": "2020-07-13T15:03:34.360273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2\n",
      "0  1.0  6.5  3.0\n",
      "1  1.0  6.5  3.0\n",
      "2  1.0  6.5  3.0\n",
      "3  1.0  6.5  3.0\n"
     ]
    }
   ],
   "source": [
    "print(data.fillna(method='backfill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **fillna** you can do lots of other things with a little creativity. For example, you might pass the mean or median value of a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:44.939601Z",
     "start_time": "2020-07-13T02:37:44.926695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    NaN\n",
       "2    3.5\n",
       "3    NaN\n",
       "4    7.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.Series([1., np.nan, 3.5, np.nan, 7])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:44.957167Z",
     "start_time": "2020-07-13T02:37:44.944663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.000000\n",
       "1    3.833333\n",
       "2    3.500000\n",
       "3    3.833333\n",
       "4    7.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:44.979187Z",
     "start_time": "2020-07-13T02:37:44.961076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        one       two     three\n",
      "a  0.186016 -0.649033 -1.412452\n",
      "b       NaN       NaN       NaN\n",
      "c  0.767773 -0.523762 -0.394308\n",
      "d       NaN       NaN       NaN\n",
      "e -0.039639  0.632435  1.329215\n",
      "f -1.113163  0.200517 -0.406525\n",
      "g       NaN       NaN       NaN\n",
      "h  1.244349  0.018936  1.295439\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randn(5, 3), index=['a', 'c', 'e', 'f',\n",
    "'h'],columns=['one', 'two', 'three'])\n",
    "\n",
    "df = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling a null values using **replace()** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:44.988456Z",
     "start_time": "2020-07-13T02:37:44.983625Z"
    }
   },
   "outputs": [],
   "source": [
    "# will replace Nan value in dataframe with value -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.010060Z",
     "start_time": "2020-07-13T02:37:44.992500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          one         two       three\n",
       "a    0.186016   -0.649033   -1.412452\n",
       "b -999.000000 -999.000000 -999.000000\n",
       "c    0.767773   -0.523762   -0.394308\n",
       "d -999.000000 -999.000000 -999.000000\n",
       "e   -0.039639    0.632435    1.329215\n",
       "f   -1.113163    0.200517   -0.406525\n",
       "g -999.000000 -999.000000 -999.000000\n",
       "h    1.244349    0.018936    1.295439"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.replace(to_replace = np.nan, value = -999) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **interpolate()** function to fill the missing values using linear method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T15:10:20.648397Z",
     "start_time": "2020-07-13T15:10:20.601011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        one       two     three\n",
      "a -1.069206 -2.390382  1.073111\n",
      "b       NaN       NaN       NaN\n",
      "c -1.172337  0.752013  0.548335\n",
      "d       NaN       NaN       NaN\n",
      "e  0.995436  0.356959  0.274437\n",
      "f -0.885913 -1.538093 -0.739337\n",
      "g       NaN       NaN       NaN\n",
      "h -0.582092 -0.263053  0.254739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        one       two     three\n",
       "a -1.069206 -2.390382  1.073111\n",
       "b -1.120772 -0.819184  0.810723\n",
       "c -1.172337  0.752013  0.548335\n",
       "d -0.088450  0.554486  0.411386\n",
       "e  0.995436  0.356959  0.274437\n",
       "f -0.885913 -1.538093 -0.739337\n",
       "g -0.734003 -0.900573 -0.242299\n",
       "h -0.582092 -0.263053  0.254739"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to interpolate the missing values \n",
    "df = pd.DataFrame(np.random.randn(5, 3), index=['a', 'c', 'e', 'f',\n",
    "'h'],columns=['one', 'two', 'three'])\n",
    "\n",
    "df = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n",
    "print(df)\n",
    "df.interpolate(method ='linear', limit_direction ='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.066237Z",
     "start_time": "2020-07-13T02:37:45.050483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        one       two     three\n",
       "a -1.069206 -2.390382  1.073111\n",
       "b -0.542823 -0.616511  0.282257\n",
       "c -1.172337  0.752013  0.548335\n",
       "d -0.542823 -0.616511  0.282257\n",
       "e  0.995436  0.356959  0.274437\n",
       "f -0.885913 -1.538093 -0.739337\n",
       "g -0.542823 -0.616511  0.282257\n",
       "h -0.582092 -0.263053  0.254739"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna(df.mean(), inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T15:15:43.274598Z",
     "start_time": "2020-07-13T15:15:43.256262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    k1  k2\n",
       "0  one   1\n",
       "1  two   1\n",
       "2  one   2\n",
       "3  two   3\n",
       "4  one   3\n",
       "5  two   4\n",
       "6  two   4"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],'k2': [1, 1, 2, 3, 3, 4, 4]})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame method <font color = 'red'>duplicated</font> returns a boolean Series indicating whether each row is a duplicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T15:12:19.470921Z",
     "start_time": "2020-07-13T15:12:19.458972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "5    False\n",
       "6     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatedly, <font color = 'red'>drop_duplicates</font> returns a DataFrame where the duplicated array is False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T15:15:55.033585Z",
     "start_time": "2020-07-13T15:15:55.020614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    k1  k2\n",
       "0  one   1\n",
       "1  two   1\n",
       "2  one   2\n",
       "3  two   3\n",
       "4  one   3\n",
       "5  two   4"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these methods by default consider all of the columns; alternatively, you can specify any subset of them to detect duplicates. Suppose we had an additional column of values and wanted to filter duplicates only based on the 'k1' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T15:12:41.775467Z",
     "start_time": "2020-07-13T15:12:41.766811Z"
    }
   },
   "outputs": [],
   "source": [
    "data['v1'] = range(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    k1  k2  v1\n",
       "0  one   1   0\n",
       "1  two   1   1\n",
       "2  one   2   2\n",
       "3  two   3   3\n",
       "4  one   3   4\n",
       "5  two   4   5\n",
       "6  two   4   6"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.165192Z",
     "start_time": "2020-07-13T02:37:45.137969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    k1  k2  v1\n",
       "0  one   1   0\n",
       "1  two   1   1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(['k1'])\n",
    "#data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretization and Binning\n",
    "Continuous data is often discretized or otherwise separated into “bins” for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T13:49:22.331990Z",
     "start_time": "2020-07-16T13:49:22.274572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\n"
     ]
    }
   ],
   "source": [
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\n",
    "print(ages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s divide these into bins of 18 to 25, 26 to 35, 36 to 60, and finally 61 and older. To do so, you have to use cut, a function in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:12:50.409294Z",
     "start_time": "2020-07-16T14:12:50.380861Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = [18, 25, 35, 60, 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:13:08.287782Z",
     "start_time": "2020-07-16T14:13:08.000420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]\n",
       "Length: 12\n",
       "Categories (4, interval[int64, right]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = pd.cut(ages,bins)\n",
    "cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object pandas returns is a special Categorical object. The output you see describes the bins computed by pandas.cut. You can treat it like an array of strings indicating the bin name; internally it contains a categories array specifying the dis‐ tinct category names along with a labeling for the ages data in the codes attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:13:46.849360Z",
     "start_time": "2020-07-16T14:13:46.813884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:15:25.538345Z",
     "start_time": "2020-07-16T14:15:25.529269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]], dtype='interval[int64, right]')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent with mathematical notation for intervals, a parenthesis means that the side is open, while the square bracket means it is closed (inclusive). You can change which side is closed by passing right=False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:17:15.622982Z",
     "start_time": "2020-07-16T14:17:15.492183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 25), [18, 25), [25, 35), [25, 35), [18, 25), ..., [25, 35), [60, 100), [35, 60), [35, 60), [25, 35)]\n",
       "Length: 12\n",
       "Categories (4, interval[int64, left]): [[18, 25) < [25, 35) < [35, 60) < [60, 100)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.cut(ages, [18, 25, 35, 60, 100], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.292919Z",
     "start_time": "2020-07-13T02:37:45.271438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 25]     5\n",
       "(25, 35]     3\n",
       "(35, 60]     3\n",
       "(60, 100]    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass your own bin names by passing a list or array to the labels option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:18:08.513192Z",
     "start_time": "2020-07-16T14:18:08.505571Z"
    }
   },
   "outputs": [],
   "source": [
    "group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:18:10.963730Z",
     "start_time": "2020-07-16T14:18:10.931683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Youth', 'Youth', 'Youth', 'YoungAdult', 'Youth', ..., 'YoungAdult', 'Senior', 'MiddleAged', 'MiddleAged', 'YoungAdult']\n",
       "Length: 12\n",
       "Categories (4, object): ['Youth' < 'YoungAdult' < 'MiddleAged' < 'Senior']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.cut(ages, bins, labels=group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass an integer number of bins to cut instead of explicit bin edges, it will compute equal-length bins based on the minimum and maximum values in the data. Consider the case of some uniformly distributed data chopped into fourths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:26:53.188525Z",
     "start_time": "2020-07-16T14:26:53.172103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04695572, 0.02492307, 0.63691027, 0.04831942, 0.12832096,\n",
       "       0.46388775, 0.98081816, 0.10972976, 0.02517424, 0.68022283,\n",
       "       0.58363332, 0.18237929, 0.61646585, 0.34944021, 0.20517741,\n",
       "       0.72336891, 0.00271105, 0.36705793, 0.6660005 , 0.11993632])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.rand(20)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:27:25.288503Z",
     "start_time": "2020-07-16T14:27:24.883781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0017, 0.25], (0.0017, 0.25], (0.49, 0.74], (0.0017, 0.25], (0.0017, 0.25], ..., (0.49, 0.74], (0.0017, 0.25], (0.25, 0.49], (0.49, 0.74], (0.0017, 0.25]]\n",
       "Length: 20\n",
       "Categories (4, interval[float64, right]): [(0.0017, 0.25] < (0.25, 0.49] < (0.49, 0.74] < (0.74, 0.98]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.cut(data, 4, precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting and Filtering Outliers\n",
    "- Outliers are data objects with characteristics that are considerably different than most of the other data objects in the data set\n",
    "\n",
    "\n",
    "- Data points inconsistent with the majority of data\n",
    "\n",
    "\n",
    "- Outlier detection can be used for fraud detection or data cleaning\n",
    "\n",
    "\n",
    "Outliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:\n",
    "\n",
    "\n",
    "- It increases the error variance and reduces the power of statistical tests\n",
    "\n",
    "\n",
    "- If the outliers are non-randomly distributed, they can decrease normality\n",
    "\n",
    "\n",
    "- They can bias or influence estimates that may be of substantive interest\n",
    "\n",
    "\n",
    "- They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to detect outliers\n",
    "\n",
    "Most commonly used method to detect outliers is visualization. \n",
    "\n",
    "We use various visualization methods, like Box-plot, Histogram, Scatter plot, clustering, curve fitting. \n",
    "\n",
    "\n",
    "Some analysts also various thumb rules to detect outliers. Some of them are:\n",
    "\n",
    "- Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR\n",
    "\n",
    "\n",
    "- Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\n",
    "\n",
    "- Data points, three or more standard deviation away from mean are considered outlier\n",
    "\n",
    "\n",
    "- Outlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding\n",
    "\n",
    "\n",
    "\n",
    "#### How to remove them \n",
    "Imputing: Like imputation of missing values, we can also impute outliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.\n",
    "\n",
    "Treat separately: If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:28:58.899245Z",
     "start_time": "2020-07-16T14:28:58.876824Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.random.randn(1000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:29:01.463494Z",
     "start_time": "2020-07-16T14:29:01.454937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:29:03.893105Z",
     "start_time": "2020-07-16T14:29:03.860487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          0         1         2         3\n",
       "0 -0.159169 -0.765983 -0.967262 -0.584910\n",
       "1 -0.308999 -0.860082 -0.873806 -0.465407\n",
       "2 -0.912744 -0.980514 -1.211883 -1.353667\n",
       "3  0.394324  2.157448  0.238104  0.074613\n",
       "4  0.688542 -1.457646  0.039135 -1.734726\n",
       "5  1.005974 -0.632832 -0.264051 -0.522218"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:29:13.683177Z",
     "start_time": "2020-07-16T14:29:13.606082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 0            1            2            3\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000\n",
       "mean      0.010833    -0.003048    -0.044633     0.032344\n",
       "std       1.027546     1.012849     0.999896     1.009377\n",
       "min      -3.314158    -3.173423    -3.057841    -2.541354\n",
       "25%      -0.658673    -0.627667    -0.766843    -0.722486\n",
       "50%       0.032462    -0.034462    -0.057484     0.029262\n",
       "75%       0.674990     0.619956     0.699498     0.722266\n",
       "max       3.096226     3.877667     3.113535     3.248491"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to find values in one of the columns exceeding 3 in absolute value:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'> do z-score transformation. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:30:33.244379Z",
     "start_time": "2020-07-16T14:30:33.233425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.967262\n",
       "1     -0.873806\n",
       "2     -1.211883\n",
       "3      0.238104\n",
       "4      0.039135\n",
       "         ...   \n",
       "995    0.165497\n",
       "996   -1.559125\n",
       "997    1.194488\n",
       "998    0.411043\n",
       "999    0.853108\n",
       "Name: 2, Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = data[2]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:30:48.429240Z",
     "start_time": "2020-07-16T14:30:48.405819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114    3.113535\n",
       "623   -3.057841\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col[np.abs(col)>3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select all rows having a value exceeding 3 or –3, you can use the any method on a boolean DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:36:15.555839Z",
     "start_time": "2020-07-16T14:36:15.528796Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/nnpgxs3d1rl0dlwj25kwmcn00000gn/T/ipykernel_42404/2969116195.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  data[(np.abs(data) > 3).any(1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "            0         1         2         3\n",
       "84  -0.258933 -1.753082 -2.001858  3.248491\n",
       "114  0.267192 -0.795920  3.113535  1.026984\n",
       "166  0.016686  3.310486  0.725794  1.779142\n",
       "227 -3.280667  1.020046 -0.365676  1.838259\n",
       "272 -3.314158  0.054977  0.060250  1.134532\n",
       "..        ...       ...       ...       ...\n",
       "608 -3.115277  0.155582  1.119411  0.606543\n",
       "623  0.085189 -0.471656 -3.057841  0.489964\n",
       "680  3.096226  0.870829  0.248657 -1.621977\n",
       "921 -0.664166  3.877667  0.599410 -0.064998\n",
       "979 -0.674065 -3.173423  0.400489  1.216701\n",
       "\n",
       "[12 rows x 4 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(np.abs(data) > 3).any(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values can be set based on these criteria. Here is code to cap values outside the inter‐ val –3 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.563341Z",
     "start_time": "2020-07-13T02:37:45.540656Z"
    }
   },
   "outputs": [],
   "source": [
    "data[np.abs(data) > 3] = np.sign(data) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.599787Z",
     "start_time": "2020-07-13T02:37:45.566008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 0            1            2            3\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000\n",
       "mean      0.011422    -0.004465    -0.044688     0.032095\n",
       "std       1.025010     1.007075     0.999370     1.008615\n",
       "min      -3.000000    -3.000000    -3.000000    -2.541354\n",
       "25%      -0.658673    -0.627667    -0.766843    -0.722486\n",
       "50%       0.032462    -0.034462    -0.057484     0.029262\n",
       "75%       0.674990     0.619956     0.699498     0.722266\n",
       "max       3.000000     3.000000     3.000000     3.000000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Indicator/Dummy Variables\n",
    "Another type of transformation for statistical modeling or machine learning applica‐ tions is converting a categorical variable into a “dummy” or “indicator” matrix. If a column in a DataFrame has k distinct values, you would derive a matrix or Data‐ Frame with k columns containing all 1s and 0s. pandas has a get_dummies function for doing this, though devising one yourself is not difficult. \n",
    "\n",
    "When you’re using statistics or machine learning tools, you’ll often transform catego‐ rical data into dummy variables, also known as one-hot encoding. This involves creat‐ ing a DataFrame with a column for each distinct category; these columns contain 1s for occurrences of a given category and 0 otherwise.\n",
    "\n",
    "Let’s return to an earlier example DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:39:30.472257Z",
     "start_time": "2020-07-16T14:39:30.456970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key  data1\n",
       "0   b      0\n",
       "1   b      1\n",
       "2   a      2\n",
       "3   c      3\n",
       "4   a      4\n",
       "5   b      5"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],'data1': range(6)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:39:38.039254Z",
     "start_time": "2020-07-16T14:39:38.023814Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.657690Z",
     "start_time": "2020-07-13T02:37:45.645098Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:40:04.796843Z",
     "start_time": "2020-07-16T14:40:04.755274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   data1  key_a  key_b  key_c\n",
       "0      0      0      1      0\n",
       "1      1      0      1      0\n",
       "2      2      1      0      0\n",
       "3      3      0      0      1\n",
       "4      4      1      0      0\n",
       "5      5      0      1      0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.725668Z",
     "start_time": "2020-07-13T02:37:45.698921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   a  b  c\n",
       "0  0  1  0\n",
       "1  0  1  0\n",
       "2  1  0  0\n",
       "3  0  0  1\n",
       "4  1  0  0\n",
       "5  0  1  0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:47:14.726140Z",
     "start_time": "2020-07-16T14:47:14.707255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   key_a  key_b  key_c\n",
       "0      0      1      0\n",
       "1      0      1      0\n",
       "2      1      0      0\n",
       "3      0      0      1\n",
       "4      1      0      0\n",
       "5      0      1      0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(df['key'], prefix='key')\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:47:18.072420Z",
     "start_time": "2020-07-16T14:47:18.042249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   data1  key_a  key_b  key_c\n",
       "0      0      0      1      0\n",
       "1      1      0      1      0\n",
       "2      2      1      0      0\n",
       "3      3      0      0      1\n",
       "4      4      1      0      0\n",
       "5      5      0      1      0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_dummy = df[['data1']].join(dummies)\n",
    "df_with_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.794445Z",
     "start_time": "2020-07-13T02:37:45.780485Z"
    }
   },
   "outputs": [],
   "source": [
    "df_with_dummy1 = df.join(dummies)\n",
    "df_with_dummy1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful recipe for statistical applications is to combine get_dummies with a discretization function like cut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.803493Z",
     "start_time": "2020-07-13T02:37:45.798370Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.818287Z",
     "start_time": "2020-07-13T02:37:45.807813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07630829, 0.77991879, 0.43840923, 0.72346518, 0.97798951,\n",
       "       0.53849587, 0.50112046, 0.07205113, 0.26843898, 0.4998825 ])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = np.random.rand(10)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.828044Z",
     "start_time": "2020-07-13T02:37:45.822260Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = [0, 0.2, 0.4, 0.6, 0.8, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:45.860171Z",
     "start_time": "2020-07-13T02:37:45.830974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   (0.0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1.0]\n",
       "0           1           0           0           0           0\n",
       "1           0           0           0           1           0\n",
       "2           0           0           1           0           0\n",
       "3           0           0           0           1           0\n",
       "4           0           0           0           0           1\n",
       "5           0           0           1           0           0\n",
       "6           0           0           1           0           0\n",
       "7           1           0           0           0           0\n",
       "8           0           1           0           0           0\n",
       "9           0           0           1           0           0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(pd.cut(values, bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining and Reshaping\n",
    "\n",
    "Once the data is tidied up,it will likely that we will then need to use this data either to combine multiple sets of data, or to reorganize the data. \n",
    "\n",
    "Here we will discuss combination and reshaping of data. Combination of data in pandas is performed by concatenating two sets of data, where data is combined simply along either axes but without regard to the relationships in the data. Or data can be combined using relationships in the data by using a pandas capability referred to as merging, which provides join operations that are similar to those in may relational databases. \n",
    "\n",
    "We will discuss various reshaping techniques like pivoting, stacking, and unstacking, and melting of data. \n",
    "\n",
    "Pivoting allows to restructure data similarly to how spreadsheets pivot data by creating new index levels and moving data into columns based upon values (or vice-versa). \n",
    "\n",
    "Stacking and unstacking are similar to pivoting, but allow us to pivot data organized with multiple levels of indexes.\n",
    "\n",
    "Finally, melting allows to restructure data into unique ID-variable-measurement combinations that are or required for many statistical analyses. Following concepts of combining and reshaping are available and will be discussed.\n",
    "\n",
    "- Concatenation\n",
    "- Merging and joining\n",
    "- Pivots\n",
    "- Stacking/unstacking\n",
    "- Melting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating\n",
    "\n",
    "Concatenating is the process of either adding rows to the end of an Series or DataFrame. The operation is performed via the function **concat**. Again the function will perform the operation on a specific axis. The general syntax is to pass a list of objects to **concat()** function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:49:28.077579Z",
     "start_time": "2020-07-16T14:49:28.061250Z"
    }
   },
   "outputs": [],
   "source": [
    "s1 = pd.Series(np.arange(1,5))\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:49:31.076339Z",
     "start_time": "2020-07-16T14:49:31.059472Z"
    }
   },
   "outputs": [],
   "source": [
    "s2 = pd.Series(np.arange(5,9))\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:49:37.936343Z",
     "start_time": "2020-07-16T14:49:37.917845Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([s1,s2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:50:21.046077Z",
     "start_time": "2020-07-16T14:50:21.033732Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([s1,s2], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two DataFrame objects can also be concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:50:50.923697Z",
     "start_time": "2020-07-16T14:50:50.897629Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.arange(9).reshape(3,3), columns = ['x','y','z'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:51:03.089138Z",
     "start_time": "2020-07-16T14:51:03.063474Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(np.arange(9,18).reshape(3,3), columns = ['x','y','z'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:51:07.144165Z",
     "start_time": "2020-07-16T14:51:07.125533Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of concatenating the two Dataframe objects will first identify the set of columns formed by aligning the labels in the columns, effectively determining the union of the column names. The resulting DataFrame object will then consists of columns, and columns with identical names will not be duplicated \n",
    "\n",
    "Rows will be then be added to the result, in the order of the each of the objects passed to function *concat()*. If a column in the result does not exist in the object being copied, NaN values will be filled in those locations. Duplicate row index labels can occur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:51:51.713372Z",
     "start_time": "2020-07-16T14:51:51.695390Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.arange(9).reshape(3,3), columns = ['a','b','c'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:51:55.875740Z",
     "start_time": "2020-07-16T14:51:55.846334Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(np.arange(9,18).reshape(3,3), columns = ['a','c','d'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:52:05.002348Z",
     "start_time": "2020-07-16T14:52:04.853034Z"
    }
   },
   "outputs": [],
   "source": [
    "# concat the two objects, but create an index using the given keys\n",
    "\n",
    "df3 = pd.concat([df1,df2])\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*concat()* function allows to specify the axis on which to apply the concatenation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.035671Z",
     "start_time": "2020-07-13T02:37:46.022020Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([df1,df2], axis =1)  # column wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the resultant dataframe contains duplicate columns. The concatenation first aligns by the row index labels of each DataFrame and then fills in the columns from the first DataFrame and then then the second. The columns are not aligned and result in duplicate values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same rules of alignment and filling on NaN values apply in thsi case except that they are applied to the rows index labels. The following example demonstrates a concatenation along the columns axis with two DataFrames that have row index labebsl in common (2 and 3) along with disjoint rows(0 in df1 and 4 in df3). Additionally, some of the columns in df3 overlap with df1(a) as well as being disjoint(d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.055042Z",
     "start_time": "2020-07-13T02:37:46.038411Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(np.arange(20,26).reshape(3,2), columns = ['a','d'], index = [2,3,4])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.082157Z",
     "start_time": "2020-07-13T02:37:46.058623Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([df1,df3], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concatenation of two or more DataFrame actually performs an outer join along the index labels on the axis opposite to the one specified. This makes the result of the concatenation similar to having performed a union of those index labels, and then data is filled based on the alignment of those labels to the source objects. \n",
    "\n",
    "\n",
    "The type of join can be changed to an inner join and can be performed by specifying *join= 'inner'* as the parameter. The inner join then logically performs the interaction instead of a union. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.100304Z",
     "start_time": "2020-07-13T02:37:46.085326Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([df1,df3], axis = 1, join = 'inner') # only 2 is the common row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use label groups of data along the columns using the *keys* parameter when applying the concatenation along axis = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.127771Z",
     "start_time": "2020-07-13T02:37:46.104073Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df1,df2], axis = 1, keys = ['df1', 'df2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**append()** method concatenate the two specified DataFrame long the row index labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:54:29.463411Z",
     "start_time": "2020-07-16T14:54:29.138236Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.append(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with a concatenation on axis =1, the index labels in the rows are copied without consideration of the creation of duplicates, and the column labels are joined in a manner which ensures no duplicate column name in included in the result. \n",
    "\n",
    "If you would like to ensure that the resulting index does not have duplicates but preserves all of the rows, you can use the *ignore_index = True* parameter. This returns the same result except with new *Int64Index* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.174222Z",
     "start_time": "2020-07-13T02:37:46.157694Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.append(df2,ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging and joining data\n",
    "\n",
    "pandas provides functionality of merging the pandas objects with database like join operations using the **merge()** function and *merge* method of a Dataframe objects.\n",
    "\n",
    "A merge combines the data of two pandas objects by finding matching values in one or more columns or row indexes. It then returns a new object that represents a combination of the data from both based on relational-databases-like join applied to those values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:55:48.755596Z",
     "start_time": "2020-07-16T14:55:48.716891Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({ 'id':[1,2,3,4,5],\n",
    "         'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'],\n",
    "         'subject_id':['sub1','sub2','sub4','sub6','sub5']})\n",
    "df2 = pd.DataFrame(\n",
    "         {'id':[1,2,3,4,5],\n",
    "         'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'],\n",
    "         'subject_id':['sub2','sub4','sub3','sub6','sub5']})\n",
    "print(df1)\n",
    "print('\\n')\n",
    "\n",
    "print(df2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:56:43.753742Z",
     "start_time": "2020-07-16T14:56:43.711848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge Two DataFrames on a key\n",
    "print(pd.merge(df1,df2,on = 'id'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.244179Z",
     "start_time": "2020-07-13T02:37:46.226684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge Two DataFrames on Multiple Keys\n",
    "print(pd.merge(df1,df2,on=['id','subject_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.262688Z",
     "start_time": "2020-07-13T02:37:46.248045Z"
    }
   },
   "outputs": [],
   "source": [
    "customers = pd.DataFrame({'CustomerID': [10,11], 'Name':['John', 'Jenny'],'Address':[\"address of john\", 'address of jenny']})\n",
    "customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.282029Z",
     "start_time": "2020-07-13T02:37:46.266310Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "orders = pd.DataFrame({'CustomerID': [10,11,10], 'Amount': [1000, 2000,5000]})\n",
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.303616Z",
     "start_time": "2020-07-13T02:37:46.285509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we would like to know the amount of the order \n",
    "# each customer spent on each order  \n",
    "\n",
    "customers.merge(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what pandas has done is the following:\n",
    "\n",
    "1. Determines the columns in both customers and orders with common labels. These columns are treated as the keys to perform the join\n",
    "\n",
    "2. It creates a new DataFrame whose columns are the labels from the keys identified from the keys identified in step 1. followed by all of the non-key labels from both objects\n",
    "\n",
    "3. It matches values in the key columns of both DataFrame objects\n",
    "\n",
    "4. It then creates a row in the result for each set of matching labels\n",
    "\n",
    "5. It then copies the data from those matching rows from each source object into that respective row and columns of the result\n",
    "\n",
    "6. it assigns a new Int64Index to the result\n",
    "\n",
    "\n",
    "The join in a merge can use values from multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.323053Z",
     "start_time": "2020-07-13T02:37:46.306948Z"
    }
   },
   "outputs": [],
   "source": [
    "left_data = {'key1':['a','b','c'], 'key2':['x','y','z'], 'lval1':['5','6','7']}\n",
    "left = pd.DataFrame(left_data,index = [0,1,2])\n",
    "\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.343988Z",
     "start_time": "2020-07-13T02:37:46.326308Z"
    }
   },
   "outputs": [],
   "source": [
    "right_data = {'key1':['a','b','c'], 'key2':['x','a','z'], 'lval1':['1','2','3']}\n",
    "\n",
    "right = pd.DataFrame(right_data, index = [1,2,3])\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.367772Z",
     "start_time": "2020-07-13T02:37:46.347510Z"
    }
   },
   "outputs": [],
   "source": [
    "left.merge(right, on = 'key1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.389890Z",
     "start_time": "2020-07-13T02:37:46.371280Z"
    }
   },
   "outputs": [],
   "source": [
    "left.merge(right, on=['key1', 'key2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns specified with on need to exist in both DataFrames. If you would like to merge based on columns with different names in each object, you can use the left_on and right_on parameters, passing the name or names of columns to each respective parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.412105Z",
     "start_time": "2020-07-13T02:37:46.394891Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(left,right,left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.444781Z",
     "start_time": "2020-07-13T02:37:46.416420Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\n",
    "                    'data1': range(7)})\n",
    "df4 = pd.DataFrame({'rkey': ['a', 'b', 'd'],\n",
    "                    'data2': range(3)})\n",
    "pd.merge(df3, df4, left_on='lkey', right_on='rkey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.478032Z",
     "start_time": "2020-07-13T02:37:46.449166Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.512812Z",
     "start_time": "2020-07-13T02:37:46.482627Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\n",
    "                    'data1': range(6)})\n",
    "df2 = pd.DataFrame({'key': ['a', 'b', 'a', 'b', 'd'],\n",
    "                    'data2': range(5)})\n",
    "df1\n",
    "df2\n",
    "pd.merge(df1, df2, on='key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.535107Z",
     "start_time": "2020-07-13T02:37:46.515668Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.569375Z",
     "start_time": "2020-07-13T02:37:46.538936Z"
    }
   },
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'key1': ['foo', 'foo', 'bar'],\n",
    "                     'key2': ['one', 'two', 'one'],\n",
    "                     'lval': [1, 2, 3]})\n",
    "right = pd.DataFrame({'key1': ['foo', 'foo', 'bar', 'bar'],\n",
    "                      'key2': ['one', 'one', 'one', 'two'],\n",
    "                      'rval': [4, 5, 6, 7]})\n",
    "pd.merge(left, right, on=['key1', 'key2'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.601773Z",
     "start_time": "2020-07-13T02:37:46.572739Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(left, right, on='key1')\n",
    "pd.merge(left, right, on='key1', suffixes=('_left', '_right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging on Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.629568Z",
     "start_time": "2020-07-13T02:37:46.605543Z"
    }
   },
   "outputs": [],
   "source": [
    "left1 = pd.DataFrame({'key': ['a', 'b', 'a', 'a', 'b', 'c'],\n",
    "                      'value': range(6)})\n",
    "right1 = pd.DataFrame({'group_val': [3.5, 7]}, index=['a', 'b'])\n",
    "left1\n",
    "right1\n",
    "pd.merge(left1, right1, left_on='key', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.655427Z",
     "start_time": "2020-07-13T02:37:46.633067Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(left1, right1, left_on='key', right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.683090Z",
     "start_time": "2020-07-13T02:37:46.658774Z"
    }
   },
   "outputs": [],
   "source": [
    "lefth = pd.DataFrame({'key1': ['Ohio', 'Ohio', 'Ohio',\n",
    "                               'Nevada', 'Nevada'],\n",
    "                      'key2': [2000, 2001, 2002, 2001, 2002],\n",
    "                      'data': np.arange(5.)})\n",
    "righth = pd.DataFrame(np.arange(12).reshape((6, 2)),\n",
    "                      index=[['Nevada', 'Nevada', 'Ohio', 'Ohio',\n",
    "                              'Ohio', 'Ohio'],\n",
    "                             [2001, 2000, 2000, 2000, 2001, 2002]],\n",
    "                      columns=['event1', 'event2'])\n",
    "lefth\n",
    "righth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.709341Z",
     "start_time": "2020-07-13T02:37:46.686808Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.735253Z",
     "start_time": "2020-07-13T02:37:46.712913Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(lefth, righth, left_on=['key1', 'key2'],\n",
    "         right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.770502Z",
     "start_time": "2020-07-13T02:37:46.738702Z"
    }
   },
   "outputs": [],
   "source": [
    "left2 = pd.DataFrame([[1., 2.], [3., 4.], [5., 6.]],\n",
    "                     index=['a', 'c', 'e'],\n",
    "                     columns=['Ohio', 'Nevada'])\n",
    "right2 = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]],\n",
    "                      index=['b', 'c', 'd', 'e'],\n",
    "                      columns=['Missouri', 'Alabama'])\n",
    "print(left2)\n",
    "print(right2)\n",
    "pd.merge(left2, right2, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.790106Z",
     "start_time": "2020-07-13T02:37:46.773850Z"
    }
   },
   "outputs": [],
   "source": [
    "left2.join(right2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.807135Z",
     "start_time": "2020-07-13T02:37:46.793712Z"
    }
   },
   "outputs": [],
   "source": [
    "left1.join(right1, on='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.830767Z",
     "start_time": "2020-07-13T02:37:46.810733Z"
    }
   },
   "outputs": [],
   "source": [
    "another = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]],\n",
    "                       index=['a', 'c', 'e', 'f'],\n",
    "                       columns=['New York', 'Oregon'])\n",
    "another\n",
    "left2.join([right2, another])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.855956Z",
     "start_time": "2020-07-13T02:37:46.834269Z"
    }
   },
   "outputs": [],
   "source": [
    "left2.join([right2, another], how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting “Wide” to “Long” Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.876766Z",
     "start_time": "2020-07-13T02:37:46.860200Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ct/b2rs611d1g96lldn5x1gj1wm0000gp/T/ipykernel_69660/730043082.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df = pd.DataFrame({'key': ['foo', 'bar', 'baz'],\n\u001b[0m\u001b[1;32m      2\u001b[0m                    \u001b[0;34m'A'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0;34m'B'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    'C': [7, 8, 9]})\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'key': ['foo', 'bar', 'baz'],\n",
    "                   'A': [1, 2, 3],\n",
    "                   'B': [4, 5, 6],\n",
    "                   'C': [7, 8, 9]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.901878Z",
     "start_time": "2020-07-13T02:37:46.880805Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ct/b2rs611d1g96lldn5x1gj1wm0000gp/T/ipykernel_69660/1782340334.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmelted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmelted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "melted = pd.melt(df, ['key'])\n",
    "melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.923593Z",
     "start_time": "2020-07-13T02:37:46.905643Z"
    }
   },
   "outputs": [],
   "source": [
    "reshaped = melted.pivot('key', 'variable', 'value')\n",
    "reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.941082Z",
     "start_time": "2020-07-13T02:37:46.927481Z"
    }
   },
   "outputs": [],
   "source": [
    "reshaped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.965298Z",
     "start_time": "2020-07-13T02:37:46.945089Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.melt(df, id_vars=['key'], value_vars=['A', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:46.994088Z",
     "start_time": "2020-07-13T02:37:46.968775Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.melt(df, value_vars=['A', 'B', 'C'])\n",
    "pd.melt(df, value_vars=['key', 'A', 'B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy\n",
    "\n",
    "Categorizing a dataset and applying a function to each group, whether an aggregation or transformation, is often a critical component of a data analysis workflow. After loading, merging, and preparing a dataset, you may need to compute group statistics or possibly pivot tables for reporting or visualization purposes. pandas provides a flexible **groupby** interface, enabling you to slice, dice, and summarize datasets in a natural way.\n",
    "\n",
    "Group operations are described by **split-apply-combine** according to *Hadley Wickham* , an author of many popular packages for the R programming language\n",
    "\n",
    "Many complex group operations can be performed in pandas. \n",
    "\n",
    "Any **groupby** operation involves one of the following operations on the original object. They are −\n",
    "\n",
    "- **Split** a pandas object into pieces using one or more keys (in the form of functions, arrays, or DataFrame column names)\n",
    "\n",
    "- Calculate group summary statistics, like count, mean, or standard deviation, or a user-defined function\n",
    "\n",
    "- Apply within-group transformations or other manipulations, like normalization, linear regression, rank, or subset selection\n",
    "\n",
    "\n",
    "- Compute pivot tables and cross-tabulations\n",
    "\n",
    "The following diagram demonstrates split-apply-combine process to sum group of numbers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.011670Z",
     "start_time": "2020-07-13T02:37:46.997162Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='split-apply-combine.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is similar to the concepts in MapReduce. As in MapReduce job is divided into pieces and distributed to many computers. Each computer then performs analysis on the set of data and calculates a result. The results are then collected and used to make decision.\n",
    "\n",
    "But in pandas, this operation differs in the scope of the data and processing. In pandas, all of the data is in memory of a single system. Because of this, it is limited to that single system's processing capabilities, but tis also makes the data analysis for that scale of data faster and more interactive in nature.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy Mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into Groups\n",
    "Pandas object can be split into any of their objects. There are multiple ways to split an object like −\n",
    "\n",
    "- <font color = 'red'>obj.groupby('key')</font>\n",
    "- <font color = 'red'>obj.groupby(['key1','key2'])</font>\n",
    "- <font color = 'red'>obj.groupby(key,axis=1)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:41:24.513194Z",
     "start_time": "2020-10-19T17:41:24.374774Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',\n",
    "         'Kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],\n",
    "         'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n",
    "         'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n",
    "         'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:45:12.833122Z",
     "start_time": "2020-10-19T17:45:12.822565Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby('Team')\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of calling *groupby()* on a DataFrame is not the actual grouped data, but a DataFrameGroupByobject. The grouping has not actually been performed. This object represents an interim description of the grouping to be performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.065082Z",
     "start_time": "2020-07-13T02:37:47.055089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the number of Groups .ngroups\n",
    "grouped.ngroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Team.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .groups property will return a Python dictionary whose keys represent the names of each group. The values in the dictionary are an array of the index labels contained within each respective group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.078510Z",
     "start_time": "2020-07-13T02:37:47.068095Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing the results of grouping\n",
    "#### Iterating Over Groups\n",
    "The GroupBy object supports iteration, generating a sequence of 2-tuples containing the group name along with the chunk of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:45:53.268557Z",
     "start_time": "2020-10-19T17:45:53.264030Z"
    }
   },
   "outputs": [],
   "source": [
    "# A helper function to print the content of the groups\n",
    "\n",
    "def print_groups (groupobject):\n",
    "    for name, group in groupobject:\n",
    "        print(name)\n",
    "        print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of multiple keys, the first element in the tuple will be a tuple of key values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:45:55.861198Z",
     "start_time": "2020-10-19T17:45:55.811034Z"
    }
   },
   "outputs": [],
   "source": [
    "for (k1, k2), group in df.groupby(['Team', 'Rank']):\n",
    "    print((k1, k2))\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:45:57.300962Z",
     "start_time": "2020-10-19T17:45:57.263992Z"
    }
   },
   "outputs": [],
   "source": [
    "pieces = dict(list(df.groupby('Team')))\n",
    "pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:45:59.541079Z",
     "start_time": "2020-10-19T17:45:59.516363Z"
    }
   },
   "outputs": [],
   "source": [
    "print_groups(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:46:03.215412Z",
     "start_time": "2020-10-19T17:46:03.196589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "\n",
    "print (df.groupby(['Team','Year']).groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:46:06.613815Z",
     "start_time": "2020-10-19T17:46:06.606227Z"
    }
   },
   "outputs": [],
   "source": [
    "# how many items in each group\n",
    "grouped.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:46:10.318320Z",
     "start_time": "2020-10-19T17:46:10.306544Z"
    }
   },
   "outputs": [],
   "source": [
    "# count of items in each column of each group\n",
    "grouped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:46:13.114395Z",
     "start_time": "2020-10-19T17:46:13.098144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve a specific group\n",
    "grouped.get_group('Devils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.291397Z",
     "start_time": "2020-07-13T02:37:47.276850Z"
    }
   },
   "outputs": [],
   "source": [
    "# The head() and tail() methods can be used to return the specified number of items in \n",
    "# each group.\n",
    "\n",
    "grouped.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.311813Z",
     "start_time": "2020-07-13T02:37:47.295278Z"
    }
   },
   "outputs": [],
   "source": [
    "# nth() method will return teh n-th item in each group. \n",
    "# First item\n",
    "grouped.nth(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.328309Z",
     "start_time": "2020-07-13T02:37:47.314885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Second item \n",
    "grouped.nth(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.343837Z",
     "start_time": "2020-07-13T02:37:47.331499Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped.nth(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:46:33.344895Z",
     "start_time": "2020-10-19T17:46:33.331159Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\n",
    "                   'key2' : ['one', 'two', 'one', 'two', 'one'],\n",
    "                   'data1' : np.random.randn(5),\n",
    "                   'data2' : np.random.randn(5)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to compute the mean of the data1 column using the labels from key1. There are a number of ways to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:48:33.565217Z",
     "start_time": "2020-10-19T17:48:33.556384Z"
    }
   },
   "outputs": [],
   "source": [
    "#grouped = df['data1'].groupby(df['key1'])\n",
    "grouped = df[['data1','data2']].groupby(df['key1'])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:48:37.439659Z",
     "start_time": "2020-10-19T17:48:37.429017Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we had passed multiple arrays as a list, we’d get something different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:52:31.096893Z",
     "start_time": "2020-10-19T17:52:31.082515Z"
    }
   },
   "outputs": [],
   "source": [
    "means = df['data1'].groupby([df['key1'], df['key2']]).mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:52:33.213893Z",
     "start_time": "2020-10-19T17:52:33.194254Z"
    }
   },
   "outputs": [],
   "source": [
    "means.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:52:44.343058Z",
     "start_time": "2020-10-19T17:52:44.329634Z"
    }
   },
   "outputs": [],
   "source": [
    "states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])\n",
    "years = np.array([2005, 2005, 2006, 2005, 2006])\n",
    "df['data1'].groupby([states, years]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:52:46.321700Z",
     "start_time": "2020-10-19T17:52:46.303819Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('key1').mean()\n",
    "df.groupby(['key1', 'key2']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the objective in using groupby, a generally useful groupby method is size, which returns a Series containing group sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:52:57.755615Z",
     "start_time": "2020-10-19T17:52:57.744169Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(['key1', 'key2']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default groupby groups on axis=0, but you can group on any of the other axes. For example, we could group the columns of our example df here by dtype like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.507973Z",
     "start_time": "2020-07-13T02:37:47.498509Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.520778Z",
     "start_time": "2020-07-13T02:37:47.512238Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "grouped = df.groupby(df.dtypes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.539872Z",
     "start_time": "2020-07-13T02:37:47.524259Z"
    }
   },
   "outputs": [],
   "source": [
    "for dtype, group in grouped:\n",
    "    print(dtype)\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Column or Subset of Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing a GroupBy object created from a DataFrame with a column name or array of column names has the effect of column subsetting for aggregation. This means that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially for large datasets, it may be desirable to aggregate only a few columns. For example, in the preceding dataset, to compute means for just the data2 column and get the result as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.561968Z",
     "start_time": "2020-07-13T02:37:47.543784Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(['key1', 'key2'])[['data2']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned by this indexing operation is a grouped DataFrame if a list or array is passed or a grouped Series if only a single column name is passed as a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:57:14.183630Z",
     "start_time": "2020-10-19T17:57:14.171100Z"
    }
   },
   "outputs": [],
   "source": [
    "s_grouped = df.groupby(['key1', 'key2'])['data2']\n",
    "s_grouped\n",
    "s_grouped.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping with Dicts and Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:58:01.096342Z",
     "start_time": "2020-10-19T17:58:01.073503Z"
    }
   },
   "outputs": [],
   "source": [
    "people = pd.DataFrame(np.random.randn(5, 5),\n",
    "                      columns=['a', 'b', 'c', 'd', 'e'],\n",
    "                      index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\n",
    "people.iloc[2:3, [1, 2]] = np.nan # Add a few NA values\n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose I have a group correspondence for the columns and want to sum together the columns by group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:58:06.363509Z",
     "start_time": "2020-10-19T17:58:06.359347Z"
    }
   },
   "outputs": [],
   "source": [
    "mapping = {'a': 'red', 'b': 'red', 'c': 'blue',\n",
    "           'd': 'blue', 'e': 'red', 'f' : 'orange'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you could construct an array from this dict to pass to groupby, but instead we can just pass the dict (I included the key 'f' to highlight that unused grouping keys are OK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:58:08.802991Z",
     "start_time": "2020-10-19T17:58:08.781915Z"
    }
   },
   "outputs": [],
   "source": [
    "by_column = people.groupby(mapping, axis=1)\n",
    "by_column.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping with Functions\n",
    "\n",
    "Using Python functions is a more generic way of defining a group mapping compared with a dict or Series. Any function passed as a group key will be called once per index value, with the return values being used as the group namesSuppose you wanted to group by the length of the names; while you could compute an array of string lengths, it’s simpler to just pass the len function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.663618Z",
     "start_time": "2020-07-13T02:37:47.645387Z"
    }
   },
   "outputs": [],
   "source": [
    "people.groupby(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.688339Z",
     "start_time": "2020-07-13T02:37:47.667915Z"
    }
   },
   "outputs": [],
   "source": [
    "key_list = ['one', 'one', 'one', 'two', 'two']\n",
    "people.groupby([len, key_list]).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by Index Levels\n",
    "\n",
    "A final convenience for hierarchically indexed datasets is the ability to aggregate using one of the levels of an axis index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.715279Z",
     "start_time": "2020-07-13T02:37:47.691866Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],\n",
    "                                    [1, 3, 5, 1, 3]],\n",
    "                                    names=['cty', 'tenor'])\n",
    "hier_df = pd.DataFrame(np.random.randn(4, 5), columns=columns)\n",
    "hier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.732979Z",
     "start_time": "2020-07-13T02:37:47.717516Z"
    }
   },
   "outputs": [],
   "source": [
    "hier_df.groupby(level='cty', axis=1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Aggregation\n",
    "\n",
    "Aggregations refer to any data transformation that produces scalar values from arrays. An aggregated function returns a single aggregated value for each group. Once the **group by**  object is created, several aggregation operations can be performed on the grouped data. \n",
    "\n",
    "The preceding examples have used several of them, including mean, count, min, and sum. You may wonder what is going on when you invoke mean() on a GroupBy object. Many common aggregations, such as those found in Table 10-1, have optimized implementations. However, you are not limited to only this set of methods.\n",
    "\n",
    "\n",
    "\n",
    "| Function name |   Description|\n",
    "|:---|:---|\n",
    "|   count | Number of non-NA values in the group |\n",
    "|   sum|  Sum of non-NA values |\n",
    "|   mean |  Mean of non-NA values |\n",
    "|   median |  Arithmetic median of non-NA values |\n",
    "|   std, var | Unbiased (n – 1 denominator) standard deviation and variance |\n",
    "|   min,max|  Minimum and maximum of non-NA values|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:04:21.003314Z",
     "start_time": "2020-10-19T18:04:20.988010Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',\n",
    "         'Kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],\n",
    "         'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n",
    "         'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n",
    "         'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:04:23.183896Z",
     "start_time": "2020-10-19T18:04:23.178539Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.groupby('Team'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:04:24.503060Z",
     "start_time": "2020-10-19T18:04:24.495446Z"
    }
   },
   "outputs": [],
   "source": [
    "## View Groups \n",
    "print(df.groupby('Team').groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:04:48.269045Z",
     "start_time": "2020-10-19T18:04:48.254976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "\n",
    "(df.groupby(['Team','Year']).groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.839240Z",
     "start_time": "2020-07-13T02:37:47.818836Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.groupby('Team')['Points'].agg(np.mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.870174Z",
     "start_time": "2020-07-13T02:37:47.844008Z"
    }
   },
   "outputs": [],
   "source": [
    "## Applying Mutliple Aggregation Functions at Once\n",
    "\n",
    "print(df.groupby('Team')['Points'].agg([np.mean, np.sum, np.std]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.960837Z",
     "start_time": "2020-07-13T02:37:47.874026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transformations\n",
    "score = lambda x: (x - x.mean()) / x.std()*10\n",
    "print(df.groupby('Team').transform(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:47.995022Z",
     "start_time": "2020-07-13T02:37:47.964891Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filtration: Return the team which have particiapted three or more times\n",
    "print(df.groupby('Team').filter(lambda x:len(x)>=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply: General split-apply-combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.036230Z",
     "start_time": "2020-07-13T02:37:47.999886Z"
    }
   },
   "outputs": [],
   "source": [
    "tips = pd.read_csv('/Users/pramodgupta/Desktop/Courses_0921/NOE/examples/tips.csv')\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.047126Z",
     "start_time": "2020-07-13T02:37:48.039509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add tip percentage of total bill\n",
    "tips['tip_pct'] = tips['tip'] / tips['total_bill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.068727Z",
     "start_time": "2020-07-13T02:37:48.051584Z"
    }
   },
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the tipping dataset from before, suppose you wanted to select the top five tip_pct values by group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.081571Z",
     "start_time": "2020-07-13T02:37:48.071986Z"
    }
   },
   "outputs": [],
   "source": [
    "def top(df, n=5, column='tip_pct'):\n",
    "    return df.sort_values(by=column)[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.109937Z",
     "start_time": "2020-07-13T02:37:48.087175Z"
    }
   },
   "outputs": [],
   "source": [
    "top(tips, n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.smoker.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.150339Z",
     "start_time": "2020-07-13T02:37:48.114135Z"
    }
   },
   "outputs": [],
   "source": [
    "tips.groupby('smoker').apply(top)  # group sby smoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.188015Z",
     "start_time": "2020-07-13T02:37:48.154356Z"
    }
   },
   "outputs": [],
   "source": [
    "result = tips.groupby('smoker')['tip_pct'].describe()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.211040Z",
     "start_time": "2020-07-13T02:37:48.191306Z"
    }
   },
   "outputs": [],
   "source": [
    "result.unstack('smoker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppressing the Group Keys\n",
    "\n",
    "You can disable this by passing group_keys=False to groupby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.247364Z",
     "start_time": "2020-07-13T02:37:48.214595Z"
    }
   },
   "outputs": [],
   "source": [
    "tips.groupby('smoker', group_keys=False).apply(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering groups\n",
    "\n",
    "\n",
    "The pandas groupby provides a *filter()* method, which can be used to make group level decisions on whether or not the entire group is included in the result after combining. The function passed to *filter()* should return *True* if the group is to be included in teh result and *False* to exclude it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.266448Z",
     "start_time": "2020-07-13T02:37:48.250721Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame ({'Label': list(\"AABCCC\"), 'Values': [1,2,3,4,np.nan,5]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are omitting which has minimum number of items. In the example we are omitting the items which have one or less items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.292805Z",
     "start_time": "2020-07-13T02:37:48.270373Z"
    }
   },
   "outputs": [],
   "source": [
    "f = lambda x:x.Values.count() >1\n",
    "df.groupby('Label').filter(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.324052Z",
     "start_time": "2020-07-13T02:37:48.296938Z"
    }
   },
   "outputs": [],
   "source": [
    "# The following will omit groups that do not have all values supplied\n",
    "f1 = lambda x:x.Values.isnull().sum()  == 0\n",
    "df.groupby('Label').filter(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:37:48.356107Z",
     "start_time": "2020-07-13T02:37:48.328970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instead of dropping a group, the use of the dropna = False parameter allows the return\n",
    "# of the offending groups, but with all their values replaced with NaN. This is useful\n",
    "# if you want to determine which items have been omitted:\n",
    "\n",
    "f = lambda x:x.Values.count() > 1\n",
    "df.groupby('Label').filter(f, dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T13:40:46.972936Z",
     "start_time": "2020-07-13T13:40:46.961186Z"
    }
   },
   "outputs": [],
   "source": [
    "d = pd.DataFrame(np.random.randn(3, 4))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T14:08:14.147257Z",
     "start_time": "2020-07-13T14:08:14.120578Z"
    }
   },
   "outputs": [],
   "source": [
    "a,b= np.where(d == np.min(d,axis =0))\n",
    "list(zip(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
